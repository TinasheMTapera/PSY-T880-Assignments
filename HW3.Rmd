---
title: 'PSY T880: HW 3'
author: "Tinashe Michael Tapera"
date: "5 February 2017"
output: html_notebook
---

# Question 1: *k*-fold cross validation.
> Explain how k-fold cross-validation is implemented.

Cross-validation (CV) is a means of estimating how well a model will perform in practice on an unknown data set. CV entails withholding part of the data set, and training a model on the given data (i.e. estimating parameters such as coefficients). In the case of *k*-fold CV, we split the data set into *k* approximately equal parts, called partitions, and then withhold a partition *k* from the model fitting stage, exactly *k* times. Each time we withhold the next *k*th partition, we fit a model to the remaining data and compute an estimate of our target evaluation/fitting statistic of choice. We can then come up with a grand estimate of model fitting, usually by averaging these estimates. Essentially, the model is exposed iteratively to all of the data, but can still be evaluated, and hence validated, as though it were naive to some of the data points.

> What are the advantages and disadvantages of k-fold cross-
validation relative to: i. The validation set approach? ii. LOOCV?

*k*-fold is a particularly useful CV approach as it allows the model to be essentially trained on all the training data one has. The validation-set approach, on the other hand, trains only on a training set, and validates only on the validation set; if the two sets are in any way statistically different, this approach will over or under-estimate model fitting statistics. On the other hand, *k*-fold CV can be more time and effort consuming than the validation set approach, especially if the validation and training set are truly randomly selected.

LOOCV is at the other extreme; *k* is set to the number of observations in the data set, and so the model is fit exactly *k*-1 times where the validation set is one data point that is left out. Again, the model fit metric is computed each time, and this means that LOOCV is extremely computationally expensive, more than *k*-fold CV. Despite this, LOOCV ensures that each data point's model fit is accounted for, meaning that the estimated model fit statistic *could be* more accurate and statistically relevant.

# Question 2: Computing estimates for standard errors
## (a)
> Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.

First, the dataset:

```{r}
library(boot)
Default = ISLR::Default
table(is.na(Default))
head(Default)
```

Looks clean. Let's run the functions:

```{r}
train = sample(length(Default$default),length(Default$default)/2)
fit1 = glm(default~ income + balance, data = Default, family = "binomial")
summary(fit1)
```

The coefficient for `income` is given by `r round(coef(fit1)[2],3)`, with standard error of $4.985 \times 10^{-6}$, while the coefficient for `balance` is given by `r round(coef(fit1)[3],3)`, with standard error of $2.274 \times 10^{-4}$.

## (b)
> Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.

```{r}
boot.fn = function(dataset, index){
  bootFit = glm(default~ income + balance, data = dataset, family = "binomial", subset = index)
  return(coef(bootFit))
}

boot.fn(Default,dim(Default)[1]*0.6)
```

## (c) 
> Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.

```{r}
boot(Default,boot.fn,100)
```

## (d) 
> Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.

The values for the estimated errors are very similar, although the bootstrapped error is slightly lower. This would ideally be a good thing, indicating a smaller margin of error for the measurement.